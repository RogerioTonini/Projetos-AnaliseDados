{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# ALURA - Desafio: 7DaysOfCode - Junho/2024\n",
    "# Processo de importação e tratamento dos dados (dias 1 e 2)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "#\n",
    "pd.options.display.max_rows = 10000\n",
    "pd.options.display.max_columns = 30\n",
    "#\n",
    "_dicNomeDF        = dict()     # Dicionário utilizado para unificar os arquivos CSV\n",
    "_intContReg       = 0          # Contador dos registros\n",
    "_lstNome_CDU      = []         # Lista que armazena temporariamente o nome da CDU\n",
    "_lstDataFrames    = []         # Lista que contém o(s) DataFrame(s) temporariamente\n",
    "_strExtArquivo    = '.csv'\n",
    "_strDB_CDU        = 'DB_CDU' + _strExtArquivo\n",
    "_strDB_Emprestimo = 'DB_Emprestimo' + _strExtArquivo\n",
    "_strDB_CadLivros  = 'DB_CadLivros' + _strExtArquivo\n",
    "_strPathData      = r'D:\\Users\\rtoni\\OneDrive\\Git-Dados\\Projetos\\7DaysOfCode.io'       # Pasta que armazena o arquivo\n",
    "_strURL_GIT       = 'https://raw.githubusercontent.com/RogerioTonini/7_Days_of_Code_Alura-Python-Pandas/main/Dia_1-Importando_dados/Datasets/dados_emprestimos/emprestimos-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# Concatena os arquivos CSV\n",
    "\"\"\"\n",
    "for _intAno in range(2010, 2021):       # Faixa de Anos que serão analisados: 2010 - 2020    \n",
    "    for _intQtdArqs in range(1, 3):     # Quantidade de arquivos por Ano = 2\n",
    "        \"\"\"\n",
    "        # _dicNomeDF  - Dicionário que armazena temporariamente os dados dos arquivos\n",
    "        # _strNomeDF  - Nome do DataFrame atual\n",
    "        # _strURL_Arq - Endereço da URL do GIT onde estão os dados\n",
    "        \"\"\"\n",
    "        _strNomeDF  = f'df_{str(_intAno)}_{str(_intQtdArqs)}'\n",
    "        _strURL_Arq = f\"{_strURL_GIT}{str(_intAno)}{str(_intQtdArqs)}{_strExtArquivo}?raw=true\"\n",
    "        print(_strNomeDF)\n",
    "        try:\n",
    "            \"\"\"\n",
    "            # Processo de tratamento dos dados Livros Emprestados:\n",
    "            # - Converte as colunas que contém Data/Hora para Datatime \n",
    "            # - Converte as colunas ['id_emprestimo'] e [matricula_e_siape] para String\n",
    "            \"\"\"\n",
    "            _dicNomeDF[_strNomeDF] = pd.read_csv(_strURL_Arq, sep=\",\")\n",
    "            _dicNomeDF[_strNomeDF]['data_emprestimo'] = pd.to_datetime(_dicNomeDF[_strNomeDF]['data_emprestimo'])\n",
    "            _dicNomeDF[_strNomeDF]['data_devolucao']  = pd.to_datetime(_dicNomeDF[_strNomeDF]['data_devolucao'])\n",
    "            _dicNomeDF[_strNomeDF]['data_renovacao']  = pd.to_datetime(_dicNomeDF[_strNomeDF]['data_renovacao'])\n",
    "        except Exception as e:\n",
    "            print(_strNomeDF, e)\n",
    "            continue\n",
    "# Unifica todos os dicionários em um único arquivo. Grava o arquivo em disco\n",
    "_lstDataFrames = []\n",
    "[ _lstDataFrames.append(df) for nome_dataframe, df in _dicNomeDF.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Verifica a quantidade total de registros duplicados e os apaga\n",
    "# Carrega os arquivos df_CadLivros e df_CDU\n",
    "# Faz o JOIN entre as tabelas df_CadLivros e df_Emprestimo\n",
    "# Acrescenta a coluna NomeFaixaCDU - Classificação Decimal Universal, conforme código da coluna df_Emprestimo['localizacao']\n",
    "\"\"\"\n",
    "try:\n",
    "    _strPathFile  = os.path.join(_strPathData, _strDB_Emprestimo)\n",
    "    df_Emprestimo = pd.concat(_lstDataFrames, ignore_index=True)     \n",
    "    df_Emprestimo.to_csv(_strPathFile, header=True, index=False, sep=';')\n",
    "    df_Emprestimo = pd.read_csv(_strPathFile, sep=';', engine='c')\n",
    "    print(f'Arquivo {_strDB_Emprestimo} gravado com sucesso!!!')\n",
    "    #\n",
    "    _strPathFile = os.path.join(_strPathData, _strDB_CadLivros)\n",
    "    df_CadLivros = pd.read_parquet('https://github.com/RogerioTonini/7_Days_of_Code_Alura-Python-Pandas/raw/main/Dia_1-Importando_dados/Datasets/dados_exemplares.parquet')\n",
    "    df_CadLivros.to_csv(_strPathFile, header=True, index=False, sep= ';')\n",
    "    #\n",
    "    df_Emprestimo.value_counts()\n",
    "    df_Emprestimo = df_Emprestimo.drop_duplicates(keep='first')   # Verifica a quantidade total de registros duplicados e os apaga\n",
    "    df_Emprestimo = df_Emprestimo.merge(df_CadLivros)             # Faz o JOIN entre as tabelas gerando o Dataframe df_Emprestimo_Tratado\n",
    "    #\n",
    "    _strPathFile = os.path.join(_strPathData, _strDB_Emprestimo)\n",
    "    df_Emprestimo.to_csv(_strPathFile, header=True, index=False, sep=';')\n",
    "    df_Emprestimo = pd.read_csv(_strPathFile, sep=';', engine='c')    \n",
    "    print(f'Arquivo {_strDB_Emprestimo}: Registros duplicados eliminados e JOIN com Cadastro de Livros realizado com sucesso!!!')\n",
    "    #\n",
    "    _strPathFile = os.path.join(_strPathData, _strDB_CDU)\n",
    "    df_CDU       = pd.read_csv(_strPathFile, sep=';', engine='c')\n",
    "    #\n",
    "    for _intCod_CDU in df_Emprestimo['localizacao']:\n",
    "        #\n",
    "        for _, _strColuna in df_CDU.iterrows():\n",
    "            if _intCod_CDU <= _strColuna['Limite_Faixa']:\n",
    "                _lstNome_CDU.append(_strColuna['Nome_Faixa_CDU'])\n",
    "                print(f'Cód. Localização: {_intCod_CDU}, Contador: {_intContReg}')\n",
    "                _intContReg += 1\n",
    "                break \n",
    "    #\n",
    "    print(f'\\n Qtde registros {_strDB_Emprestimo}, {str(_intContReg)}, Qtde lista _lstNome_CDU: {str(len(_lstNome_CDU))} \\n')\n",
    "    df_Emprestimo['Nome_Faixa_CDU']  = _lstNome_CDU\n",
    "    df_Emprestimo['matricula_siape'] = df_Emprestimo['matricula_ou_siape'].astype(str)\n",
    "    df_Emprestimo.drop(columns=['matricula_ou_siape', 'localizacao', 'registro_sistema'], inplace = True)  # Exclusão colunas: ['matricula_ou_siape', 'localizacao', 'registro_sistema']\n",
    "    #\n",
    "    _strPathFile = os.path.join(_strPathData, _strDB_Emprestimo)\n",
    "    df_Emprestimo.to_csv(_strPathFile, header=True, index=False, sep=';')\n",
    "    print('Processo de tratamento/geração do arquivo tratado finalizado!!!')\n",
    "except OSError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
